{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wf7__9oAT5K",
        "outputId": "182252d7-8657-4f2b-85b7-e45b14f46461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/mnt\n"
          ]
        }
      ],
      "source": [
        "# Setup and few imports\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mnt')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink(r'/content/mnt/MyDrive/Code_for_bearing_diagnosis/', nb_path)#Link folder\n",
        "sys.path.insert(0, nb_path)\n",
        "nb_path2 = '/content/Fed_learning_functions'\n",
        "os.symlink(r'/content/mnt/MyDrive/Fed_learning_functions/', nb_path2)#Link folder\n",
        "sys.path.insert(0, nb_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zCX4c4bLDXN",
        "outputId": "e666757c-6da3-4cda-890f-a0d669983dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf-models-official\n",
            "  Downloading tf_models_official-2.11.5-py2.py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting immutabledict\n",
            "  Downloading immutabledict-2.2.3-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (4.7.0.72)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (1.16.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (2.70.0)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (0.29.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (3.7.1)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (0.13.0)\n",
            "Collecting tensorflow-text~=2.11.0\n",
            "  Downloading tensorflow_text-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (1.4.4)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (1.22.4)\n",
            "Requirement already satisfied: tensorflow~=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (2.11.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (8.4.0)\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (5.9.4)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (2.0.6)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (1.10.1)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (4.1.3)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting pyyaml<6.0,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m630.1/630.1 KB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (4.8.3)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (1.5.13)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (1.1.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.11.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (4.1.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.1.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.16.2)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.21.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle>=1.3.9->tf-models-official) (2022.12.7)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle>=1.3.9->tf-models-official) (1.26.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle>=1.3.9->tf-models-official) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle>=1.3.9->tf-models-official) (8.0.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.22.0->tf-models-official) (2022.7.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (3.8.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (1.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (3.3.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (2.11.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (2.11.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (15.0.6.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (2.11.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (23.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (0.31.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (1.51.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (23.3.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (4.5.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (3.19.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (2.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow~=2.11.0->tf-models-official) (67.6.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.8)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->tf-models-official) (5.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->tf-models-official) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->tf-models-official) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->tf-models-official) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->tf-models-official) (4.39.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->tf-models-official) (1.0.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.9/dist-packages (from oauth2client->tf-models-official) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from oauth2client->tf-models-official) (4.9)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from oauth2client->tf-models-official) (0.4.8)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from sacrebleu->tf-models-official) (4.9.2)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/dist-packages (from sacrebleu->tf-models-official) (0.8.10)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from sacrebleu->tf-models-official) (2022.10.31)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.9/dist-packages (from seqeval->tf-models-official) (1.2.2)\n",
            "Collecting typeguard>=2.7\n",
            "  Downloading typeguard-3.0.2-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->tf-models-official) (1.12.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->tf-models-official) (8.1.3)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->tf-models-official) (1.1.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow~=2.11.0->tf-models-official) (0.40.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official) (3.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.59.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official) (5.3.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle>=1.3.9->tf-models-official) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (3.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (2.2.3)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow-addons->tf-models-official) (6.1.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (2.1.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (3.2.2)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=dad8fed7efee5e7b46918c1338bbf28f9998345615d7ba0661ea86a2a3cf0f2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/a5/92/2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
            "Successfully built seqeval\n",
            "Installing collected packages: sentencepiece, py-cpuinfo, tensorflow-model-optimization, pyyaml, portalocker, immutabledict, colorama, typeguard, sacrebleu, tensorflow-addons, seqeval, tensorflow-text, tf-models-official\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "Successfully installed colorama-0.4.6 immutabledict-2.2.3 portalocker-2.7.0 py-cpuinfo-9.0.0 pyyaml-5.4.1 sacrebleu-2.3.1 sentencepiece-0.1.97 seqeval-1.2.2 tensorflow-addons-0.19.0 tensorflow-model-optimization-0.7.3 tensorflow-text-2.11.0 tf-models-official-2.11.5 typeguard-3.0.2\n"
          ]
        }
      ],
      "source": [
        "pip install tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Pey97ALeLEg5",
        "outputId": "e67a385d-9fa0-4d7e-c75a-6f7ff1b807d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pylab as plt\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "#Related functions.\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "import official.nlp.modeling.layers as nlp_layers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling2D,AveragePooling1D\n",
        "from tensorflow.keras.layers import Activation,BatchNormalization\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K\n",
        "import openpyxl as op\n",
        "from scipy.io import savemat,loadmat\n",
        "import pickle#Use pickle to save model\n",
        "from sklearn.utils import shuffle\n",
        "from numpy.random import seed\n",
        "from tensorflow.keras.utils import set_random_seed\n",
        "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "os.path.dirname(current_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIpod0g0LFrM"
      },
      "outputs": [],
      "source": [
        "data_path = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))+\"\\notebooks\\Paderborn_case_study\\data\"\n",
        "sys.path.append(r'/content/mnt/MyDrive/Fed_learning_functions/')\n",
        "\n",
        "from StatisticsV2 import NormalDist\n",
        "from Dataset_pre import *\n",
        "from FedModels import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ufi3krCxl1I9"
      },
      "outputs": [],
      "source": [
        "tf.config.list_physical_devices('GPU')\n",
        "client_number=12\n",
        "sample_len=512\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 4\n",
        "\n",
        "num_hidden = 64\n",
        "num_layers= 3\n",
        "lr = 0.0005\n",
        "lr_decay=0\n",
        "comms_round = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDKEdZGFLG3m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "total_train_dataset=list(np.load('/content/mnt/MyDrive/Code_for_bearing_diagnosis/ISUtraining_dataset_UB.npy',allow_pickle=True))\n",
        "total_test_dataset=list(np.load('/content/mnt/MyDrive/Code_for_bearing_diagnosis/ISUtest_dataset_UB.npy',allow_pickle=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lp6g49v0YLW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "data_client1=subset_dataset(total_train_dataset[0],1.0)\n",
        "data_client2=subset_dataset(total_train_dataset[1],1.0)\n",
        "data_client3=subset_dataset(total_train_dataset[2],1.0)\n",
        "\n",
        "data_client4=subset_dataset(total_train_dataset[3],1.0)\n",
        "data_client5=subset_dataset(total_train_dataset[4],1.0)\n",
        "data_client6=subset_dataset(total_train_dataset[5],1.0)\n",
        "\n",
        "\n",
        "data_client7=subset_dataset(total_train_dataset[6],1.0)\n",
        "data_client8=subset_dataset(total_train_dataset[7],1.0)\n",
        "data_client9=subset_dataset(total_train_dataset[8],1.0)\n",
        "\n",
        "\n",
        "data_client10=subset_dataset(total_train_dataset[9],1.0)\n",
        "data_client11=subset_dataset(total_train_dataset[10],1.0)\n",
        "data_client12=subset_dataset(total_train_dataset[11],1.0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "total_train_dataset=[data_client1,data_client2,data_client3,\n",
        "                    data_client4,data_client5,data_client6,\n",
        "                    data_client7,data_client8,data_client9,\n",
        "                    data_client10,data_client11,data_client12]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHLNX707Y87A"
      },
      "outputs": [],
      "source": [
        "balanced_test_dataset=[\n",
        "np.row_stack([total_test_dataset[0],total_test_dataset[1][np.where(total_test_dataset[1][:,-1]==2)[0]],total_test_dataset[2][np.where(total_test_dataset[2][:,-1]==3)[0]]]),\n",
        "np.row_stack([total_test_dataset[0],total_test_dataset[1][np.where(total_test_dataset[1][:,-1]==2)[0]],total_test_dataset[2][np.where(total_test_dataset[2][:,-1]==3)[0]]]),\n",
        "np.row_stack([total_test_dataset[0],total_test_dataset[1][np.where(total_test_dataset[1][:,-1]==2)[0]],total_test_dataset[2][np.where(total_test_dataset[2][:,-1]==3)[0]]]),\n",
        "\n",
        "\n",
        "np.row_stack([total_test_dataset[3],total_test_dataset[4][np.where(total_test_dataset[4][:,-1]==2)[0]],total_test_dataset[5][np.where(total_test_dataset[5][:,-1]==3)[0]]]),\n",
        "np.row_stack([total_test_dataset[3],total_test_dataset[4][np.where(total_test_dataset[4][:,-1]==2)[0]],total_test_dataset[5][np.where(total_test_dataset[5][:,-1]==3)[0]]]),\n",
        "np.row_stack([total_test_dataset[3],total_test_dataset[4][np.where(total_test_dataset[4][:,-1]==2)[0]],total_test_dataset[5][np.where(total_test_dataset[5][:,-1]==3)[0]]]),\n",
        "\n",
        "\n",
        "np.row_stack([total_test_dataset[6],total_test_dataset[7][np.where(total_test_dataset[7][:,-1]==2)[0]],total_test_dataset[8][np.where(total_test_dataset[8][:,-1]==3)[0]]]),\n",
        "np.row_stack([total_test_dataset[6],total_test_dataset[7][np.where(total_test_dataset[7][:,-1]==2)[0]],total_test_dataset[8][np.where(total_test_dataset[8][:,-1]==3)[0]]]),\n",
        "np.row_stack([total_test_dataset[6],total_test_dataset[7][np.where(total_test_dataset[7][:,-1]==2)[0]],total_test_dataset[8][np.where(total_test_dataset[8][:,-1]==3)[0]]]),\n",
        "\n",
        "np.row_stack([total_test_dataset[9],total_test_dataset[10][np.where(total_test_dataset[10][:,-1]==2)[0]],total_test_dataset[11][np.where(total_test_dataset[11][:,-1]==3)[0]]]),\n",
        "np.row_stack([total_test_dataset[9],total_test_dataset[10][np.where(total_test_dataset[10][:,-1]==2)[0]],total_test_dataset[11][np.where(total_test_dataset[11][:,-1]==3)[0]]]),\n",
        "np.row_stack([total_test_dataset[9],total_test_dataset[10][np.where(total_test_dataset[10][:,-1]==2)[0]],total_test_dataset[11][np.where(total_test_dataset[11][:,-1]==3)[0]]]),\n",
        "\n",
        "\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-5UWdyMUnjs",
        "outputId": "ffe55cb0-0705-4a23-e7ba-354a1f076fbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "np.unique(total_train_dataset[5][:,-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXAT999sLLPj"
      },
      "outputs": [],
      "source": [
        "def Fed_Avg(runs):\n",
        "  for data_split_ratio in [1]:#[0.2,0.4,0.6,0.8,1]:\n",
        "      sub_train_dataset=[]\n",
        "      for i in total_train_dataset:\n",
        "          sub_train_dataset.append(subset_dataset(i,data_split_ratio))\n",
        "      #Train model, 20% as validation data to check if model is overfitted.\n",
        "      total_epoch=300\n",
        "      seed(30+runs)\n",
        "      set_random_seed(30+runs)\n",
        "      tf.random.set_seed(30+runs)\n",
        "      resnet_config = dict(num_classes=num_classes, num_layers=num_layers, num_hidden=num_hidden)\n",
        "      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "      metrics = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "      optimizer = SGD(learning_rate=lr,\n",
        "                  # decay=lr_decay,\n",
        "                  momentum=0.9\n",
        "                  )\n",
        "      # optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "      train_config = dict(loss=loss, metrics=metrics, optimizer=optimizer)\n",
        "      local_model = DeepResNetSNGPWithCovReset(**resnet_config)\n",
        "      local_model.compile(**train_config)\n",
        "      local_model.build((None,sample_len))\n",
        "      model_weights = local_model.get_weights()\n",
        "      Summary_of_local_weights=[model_weights,model_weights,model_weights,model_weights,\n",
        "                                model_weights,model_weights,model_weights,model_weights,\n",
        "                                model_weights,model_weights,model_weights,model_weights,\n",
        "                                ]\n",
        "      Updated_local_weights=[model_weights,model_weights,model_weights,model_weights,\n",
        "                            model_weights,model_weights,model_weights,model_weights,\n",
        "                            model_weights,model_weights,model_weights,model_weights,\n",
        "                            ]\n",
        "      #Revise the following part: training epoch for each model = 50. After each epoch, each client load model and calculate and generate matrix.\n",
        "\n",
        "      #The size of each matrix= [50,12,12]\n",
        "      Matrix_acc_training=np.zeros([100,12,12])\n",
        "      Matrix_acc_test=np.zeros([100,12,12])\n",
        "      Matrix_acc_balanced=np.zeros([100,12,12])\n",
        "      Matrix_acc=np.zeros([100,12,12])\n",
        "      Matrix_loss_training=np.zeros([100,12,12])\n",
        "\n",
        "      training_epoch=0\n",
        "      Cluster_set_r=[]\n",
        "      for Dataset_ID in range(client_number):\n",
        "          Model_ID= Dataset_ID\n",
        "\n",
        "          local_training=sub_train_dataset[Dataset_ID]\n",
        "          local_test=total_test_dataset[Dataset_ID]\n",
        "          local_test_balanced=balanced_test_dataset[Dataset_ID]\n",
        "\n",
        "          local_model.set_weights(Summary_of_local_weights[Model_ID])\n",
        "\n",
        "          #Test and calculate matrie\n",
        "          Model_loss,Model_acc=local_model.evaluate(local_training[:,:sample_len],local_training[:,sample_len], verbose=0)\n",
        "          Matrix_acc_training[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "          Matrix_loss_training[training_epoch,Dataset_ID,Model_ID]=Model_loss\n",
        "\n",
        "          Model_loss,Model_acc=local_model.evaluate(local_test[:,:sample_len],local_test[:,sample_len], verbose=0)\n",
        "          Matrix_acc_test[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "\n",
        "\n",
        "          Model_loss,Model_acc=local_model.evaluate(local_test_balanced[:,:sample_len],local_test_balanced[:,sample_len], verbose=0)\n",
        "          Matrix_acc_balanced[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "\n",
        "      for training_epoch in range(1,50):\n",
        "          for client in range(client_number):\n",
        "              local_training=sub_train_dataset[client]\n",
        "              local_model.set_weights(Summary_of_local_weights[client])\n",
        "              local_model.fit(local_training[:,:sample_len],local_training[:,sample_len],batch_size=batch_size, epochs=5, verbose=0)\n",
        "              Summary_of_local_weights[client]=local_model.get_weights()\n",
        "\n",
        "          print(str(training_epoch)+ 'done')\n",
        "\n",
        "\n",
        "          cluster_set=[0,1,2,3,4,5,6,7,8,9,10,11]\n",
        "          print('Model_agg')\n",
        "\n",
        "          Total_sample_number=np.shape(sub_train_dataset[0])[0]\n",
        "          Updated_local_weights=np.multiply(Summary_of_local_weights[0],Total_sample_number)\n",
        "          for Model_ID in cluster_set[1::]:\n",
        "\n",
        "              Sample_number=np.shape(sub_train_dataset[Model_ID])[0]\n",
        "              Total_sample_number= Total_sample_number+Sample_number\n",
        "              Updated_local_weights=Updated_local_weights+np.multiply(Summary_of_local_weights[Model_ID],Sample_number)\n",
        "\n",
        "          for ith_Model in cluster_set:\n",
        "              Summary_of_local_weights[ith_Model]=np.divide(Updated_local_weights, Total_sample_number)\n",
        "\n",
        "\n",
        "          #Verify model performance\n",
        "\n",
        "          for Dataset_ID in range(client_number):\n",
        "              local_training=sub_train_dataset[Dataset_ID]\n",
        "              local_test=total_test_dataset[Dataset_ID]\n",
        "              local_test_balanced=balanced_test_dataset[Dataset_ID]\n",
        "\n",
        "              for Model_ID in range(client_number):\n",
        "                  local_model.set_weights(Summary_of_local_weights[Model_ID])\n",
        "\n",
        "                  #Test and calculate matrie\n",
        "\n",
        "                  Model_loss,Model_acc=local_model.evaluate(local_training[:,:sample_len],local_training[:,sample_len], verbose=0)\n",
        "                  Matrix_acc_training[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "                  Matrix_loss_training[training_epoch,Dataset_ID,Model_ID]=Model_loss\n",
        "\n",
        "\n",
        "                  Model_loss,Model_acc=local_model.evaluate(local_test[:,:sample_len],local_test[:,sample_len], verbose=0)\n",
        "                  Matrix_acc_test[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "\n",
        "\n",
        "                  Model_loss,Model_acc=local_model.evaluate(local_test_balanced[:,:sample_len],local_test_balanced[:,sample_len], verbose=0)\n",
        "                  Matrix_acc_balanced[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "          print(np.diagonal(Matrix_acc_training[training_epoch]))\n",
        "          print('Test:')\n",
        "          print(np.diagonal(Matrix_acc_test[training_epoch]))\n",
        "          print('Bal:')\n",
        "          print(np.diagonal(Matrix_acc_balanced[training_epoch]))\n",
        "\n",
        "\n",
        "  save_data_dir_root=\"/content/mnt/MyDrive/Code_for_bearing_diagnosis/ISUresults_SC3/Run\"+str(runs)+\"/Fedavg_all_samples_\"+str(data_split_ratio)\n",
        "  if not os.path.exists(save_data_dir_root):\n",
        "      os.makedirs(save_data_dir_root)\n",
        "  os.chdir(save_data_dir_root)\n",
        "\n",
        "  with open('Summary_of_all_model','wb') as fp:\n",
        "      pickle.dump(Summary_of_local_weights,fp)\n",
        "  savemat('Model_training.mat', {'training_loss':Matrix_loss_training,'training_acc':Matrix_acc_training})\n",
        "  savemat('Model_test_all.mat', {'Test_IID':Matrix_acc_test,'Test_NonIID':Matrix_acc_balanced})\n",
        "\n",
        "  with open('Cluster_set.pkl', 'wb') as f:\n",
        "      pickle.dump(Cluster_set_r, f)\n",
        "  finalc = np.empty(len(Cluster_set_r),dtype=object)\n",
        "  finalc[:]=Cluster_set_r\n",
        "  np.save('FedavgCluster_set_r.npy', Cluster_set_r, allow_pickle=True)\n",
        "  return print('Fedavg_done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYJB2QNAaupp"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Start Fedclustering model training\n",
        "runs=0\n",
        "def Fed_SNGP(runs):\n",
        "    for data_split_ratio in [1]:#[0.2,0.4,0.6,0.8,1]:\n",
        "\n",
        "        sub_train_dataset=[]\n",
        "        for i in total_train_dataset:\n",
        "            sub_train_dataset.append(subset_dataset(i,data_split_ratio))\n",
        "\n",
        "        total_epoch=300\n",
        "        seed(30+runs)\n",
        "        set_random_seed(30+runs)\n",
        "        tf.random.set_seed(30+runs)\n",
        "\n",
        "\n",
        "        local_training=sub_train_dataset[0]\n",
        "        resnet_config = dict(num_classes=num_classes, num_layers=num_layers, num_hidden=num_hidden)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        metrics = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "        optimizer = SGD(learning_rate=lr,\n",
        "                    # decay=lr_decay,\n",
        "                    momentum=0.9\n",
        "                    )\n",
        "\n",
        "        train_config = dict(loss=loss, metrics=metrics, optimizer=optimizer)\n",
        "        local_model = DeepResNetSNGPWithCovReset(**resnet_config)\n",
        "        local_model.compile(**train_config)\n",
        "        local_model.build((None,sample_len))\n",
        "        model_weights = local_model.get_weights()\n",
        "        Summary_of_local_weights=[model_weights,model_weights,model_weights,model_weights,\n",
        "                                  model_weights,model_weights,model_weights,model_weights,\n",
        "                                  model_weights,model_weights,model_weights,model_weights,\n",
        "                                  ]\n",
        "        Updated_local_weights=[model_weights,model_weights,model_weights,model_weights,\n",
        "                              model_weights,model_weights,model_weights,model_weights,\n",
        "                              model_weights,model_weights,model_weights,model_weights,\n",
        "                              ]\n",
        "\n",
        "        #The size of each matrix= [50,12,12]\n",
        "\n",
        "        Matrix_acc_training=np.zeros([100,12,12])\n",
        "        Matrix_acc_test=np.zeros([100,12,12])\n",
        "        Matrix_acc_balanced=np.zeros([100,12,12])\n",
        "        Matrix_acc=np.zeros([100,12,12])\n",
        "        Matrix_loss_training=np.zeros([100,12,12])\n",
        "        Matrix_1=np.zeros([100,12,12])\n",
        "        Matrix_1_new=np.zeros([100,12,12])\n",
        "        training_epoch=0\n",
        "        Cluster_set_r=[]\n",
        "        for Dataset_ID in range(client_number):\n",
        "\n",
        "            Model_ID= Dataset_ID\n",
        "            local_training=sub_train_dataset[Dataset_ID]\n",
        "            local_test=total_test_dataset[Dataset_ID]\n",
        "            local_test_balanced=balanced_test_dataset[Dataset_ID]\n",
        "\n",
        "            local_model.set_weights(Summary_of_local_weights[Model_ID])\n",
        "\n",
        "            #Test and calculate matrie\n",
        "\n",
        "            Model_loss,Model_acc=local_model.evaluate(local_training[:,:sample_len],local_training[:,sample_len], verbose=0)\n",
        "            Matrix_acc_training[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "            Matrix_loss_training[training_epoch,Dataset_ID,Model_ID]=Model_loss\n",
        "\n",
        "            Model_loss,Model_acc=local_model.evaluate(local_test[:,:sample_len],local_test[:,sample_len], verbose=0)\n",
        "            Matrix_acc_test[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "\n",
        "\n",
        "            Model_loss,Model_acc=local_model.evaluate(local_test_balanced[:,:sample_len],local_test_balanced[:,sample_len], verbose=0)\n",
        "            Matrix_acc_balanced[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "\n",
        "        for training_epoch in range(1,50):\n",
        "\n",
        "            for client in range(client_number):\n",
        "                local_training=sub_train_dataset[client]\n",
        "                local_model.set_weights(Summary_of_local_weights[client])\n",
        "                local_model.fit(local_training[:,:sample_len],local_training[:,sample_len],batch_size=batch_size, epochs=5, verbose=0)\n",
        "                Summary_of_local_weights[client]=local_model.get_weights()\n",
        "                print('Client' + str(Dataset_ID)+ 'done')\n",
        "\n",
        "\n",
        "            for Dataset_ID in range(client_number):\n",
        "                local_eva=sub_train_dataset[Dataset_ID] #Only check training data.\n",
        "\n",
        "                for Model_ID in range(client_number):\n",
        "\n",
        "                    local_model.set_weights(Summary_of_local_weights[Model_ID])\n",
        "\n",
        "                    #Test and calculate matrie\n",
        "                    test_logits, test_covmat = local_model(local_eva[:,:sample_len], return_covmat=True)\n",
        "                    test_probs = np.diag(test_covmat)\n",
        "\n",
        "                    Model_acc=accuracy_score(tf.argmax(test_logits, axis=1), local_eva[:,sample_len])\n",
        "\n",
        "                    sngp_variance = tf.linalg.diag_part(test_covmat)[:, None]\n",
        "                    sngp_logits_adjusted = test_logits / tf.sqrt(1. + (np.pi / 8.) * sngp_variance)\n",
        "\n",
        "                    sngp_probs = tf.nn.softmax(sngp_logits_adjusted, axis=-1)\n",
        "\n",
        "                    label_arr=np.array(range(num_classes))+1\n",
        "\n",
        "                    Matrix_1[training_epoch,Dataset_ID,Model_ID]=np.mean(np.sum(label_arr**2*sngp_probs,1)-(np.sum(label_arr*sngp_probs,1))**2)\n",
        "\n",
        "                    #Use matrix_1 to identify ...\n",
        "                    #Normalize Matrix_1\n",
        "                    #Dataset_ID row of Matrix_1[epoch], when Dataset_ID = Model_ID, if the training acc < 80%, the Model hasn't acquire enough knowledge.\n",
        "                    # for Dataset_ID in range(client_number): This row is used for debug.\n",
        "                    #Calculate weighting matrix.\n",
        "\n",
        "            for Dataset_ID in range (client_number):\n",
        "                Uncertainty_max = np.max(Matrix_1[training_epoch,:,Dataset_ID])\n",
        "                Uncertainty_min = np.min(Matrix_1[training_epoch,:,Dataset_ID])\n",
        "                Matrix_1_new[training_epoch,:,Dataset_ID]=(Matrix_1[training_epoch,:,Dataset_ID]-(Uncertainty_min))/(Uncertainty_max-Uncertainty_min)\n",
        "\n",
        "\n",
        "\n",
        "            cluster_set=[]\n",
        "            clustering = AffinityPropagation(random_state=5).fit(Matrix_1_new[training_epoch])\n",
        "            Type_of_labels=np.unique(clustering.labels_)\n",
        "            for Type_label in Type_of_labels:\n",
        "                cluster_set.append(list(np.where(clustering.labels_==Type_label)[0]))\n",
        "            Cluster_set_r.append(cluster_set)\n",
        "            print('Clustering_results:')\n",
        "            print(cluster_set)\n",
        "\n",
        "\n",
        "            #Model aggregation\n",
        "            for sub_cluster in cluster_set:\n",
        "                if len(sub_cluster) !=1:\n",
        "                    print('Model_agg')\n",
        "                    Total_sample_number=np.shape(sub_train_dataset[sub_cluster[0]])[0]\n",
        "                    Updated_local_weights[sub_cluster[0]]=np.multiply(Summary_of_local_weights[sub_cluster[0]],Total_sample_number)\n",
        "                    for Model_ID in sub_cluster[1::]:\n",
        "                        Sample_number=np.shape(sub_train_dataset[Model_ID])[0]\n",
        "                        Total_sample_number= Total_sample_number+Sample_number\n",
        "                        Updated_local_weights[sub_cluster[0]]=Updated_local_weights[sub_cluster[0]]+np.multiply(Summary_of_local_weights[Model_ID],Sample_number)\n",
        "                    for ith_Model in sub_cluster:\n",
        "                        Summary_of_local_weights[ith_Model]=np.divide(Updated_local_weights[sub_cluster[0]], Total_sample_number)\n",
        "\n",
        "\n",
        "            #Verify model performance\n",
        "            for Dataset_ID in range(client_number):\n",
        "                local_training=sub_train_dataset[Dataset_ID]\n",
        "                local_test=total_test_dataset[Dataset_ID]\n",
        "                local_test_balanced=balanced_test_dataset[Dataset_ID]\n",
        "\n",
        "                for Model_ID in range(client_number):\n",
        "                    local_model.set_weights(Summary_of_local_weights[Model_ID])\n",
        "\n",
        "                    #Test and calculate matrie\n",
        "\n",
        "                    Model_loss,Model_acc=local_model.evaluate(local_training[:,:sample_len],local_training[:,sample_len], verbose=0)\n",
        "                    Matrix_acc_training[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "                    Matrix_loss_training[training_epoch,Dataset_ID,Model_ID]=Model_loss\n",
        "\n",
        "\n",
        "                    Model_loss,Model_acc=local_model.evaluate(local_test[:,:sample_len],local_test[:,sample_len], verbose=0)\n",
        "                    Matrix_acc_test[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "\n",
        "\n",
        "                    Model_loss,Model_acc=local_model.evaluate(local_test_balanced[:,:sample_len],local_test_balanced[:,sample_len], verbose=0)\n",
        "                    Matrix_acc_balanced[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "            print(np.diagonal(Matrix_acc_training[training_epoch]))\n",
        "            print('Test:')\n",
        "            print(np.diagonal(Matrix_acc_test[training_epoch]))\n",
        "            print('Bal:')\n",
        "            print(np.diagonal(Matrix_acc_balanced[training_epoch]))\n",
        "\n",
        "    save_data_dir_root=\"/content/mnt/MyDrive/Code_for_bearing_diagnosis/ISUresults_SC3/Run\"+str(runs)+\"/FedSNGP_all_samples_\"+str(data_split_ratio)\n",
        "    if not os.path.exists(save_data_dir_root):\n",
        "        os.makedirs(save_data_dir_root)\n",
        "    os.chdir(save_data_dir_root)\n",
        "\n",
        "    with open('Summary_of_all_model','wb') as fp:\n",
        "        pickle.dump(Summary_of_local_weights,fp)\n",
        "    savemat('Model_training.mat', {'training_loss':Matrix_loss_training,'training_acc':Matrix_acc_training})\n",
        "    savemat('Model_test_all.mat', {'Test_IID':Matrix_acc_test,'Test_NonIID':Matrix_acc_balanced})\n",
        "    savemat('Model_matrix.mat', {'Matrix1':Matrix_1_new})\n",
        "\n",
        "    with open('Cluster_set.pkl', 'wb') as f:\n",
        "        pickle.dump(Cluster_set_r, f)\n",
        "    finalc = np.empty(len(Cluster_set_r),dtype=object)\n",
        "    finalc[:]=Cluster_set_r\n",
        "    np.save('FedSNGPCluster_set_r.npy', Cluster_set_r, allow_pickle=True)\n",
        "    return print('FedSNGP_done')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Erg2pFOMMndA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-61A5RhJtee"
      },
      "source": [
        "Cosine_sim_fED\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjn9rgEGJsgg"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial import distance\n",
        "def Cal_model_dis(A,B):\n",
        "    Par1=[]\n",
        "    Par2=[]\n",
        "    for i in range(len(A)):\n",
        "        Par1.append(A[i].reshape(-1,1))\n",
        "        Par2.append(B[i].reshape(-1,1))\n",
        "    Par1=np.concatenate(Par1).reshape(-1)\n",
        "    Par2=np.concatenate(Par2).reshape(-1)\n",
        "\n",
        "    return distance.cosine(Par1,Par2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Fed_Cos_2(runs):\n",
        "    for data_split_ratio in [1]:#[0.2,0.4,0.6,0.8,1]:\n",
        "\n",
        "        sub_train_dataset=[]\n",
        "        for i in total_train_dataset:\n",
        "            sub_train_dataset.append(subset_dataset(i,data_split_ratio))\n",
        "\n",
        "        total_epoch=300\n",
        "        seed(30+runs)\n",
        "        set_random_seed(30+runs)\n",
        "        tf.random.set_seed(30+runs)\n",
        "\n",
        "\n",
        "        local_training=sub_train_dataset[0]\n",
        "        resnet_config = dict(num_classes=num_classes, num_layers=num_layers, num_hidden=num_hidden)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        metrics = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "        optimizer = SGD(learning_rate=lr,\n",
        "                    # decay=lr_decay,\n",
        "                    momentum=0.9\n",
        "                    )\n",
        "\n",
        "        train_config = dict(loss=loss, metrics=metrics, optimizer=optimizer)\n",
        "        local_model = DeepResNetSNGPWithCovReset(**resnet_config)\n",
        "        local_model.compile(**train_config)\n",
        "        local_model.build((None,sample_len))\n",
        "        model_weights = local_model.get_weights()\n",
        "        Summary_of_local_weights=[model_weights,model_weights,model_weights,model_weights,\n",
        "                                  model_weights,model_weights,model_weights,model_weights,\n",
        "                                  model_weights,model_weights,model_weights,model_weights,\n",
        "                                  ]\n",
        "        Updated_local_weights=[model_weights,model_weights,model_weights,model_weights,\n",
        "                              model_weights,model_weights,model_weights,model_weights,\n",
        "                              model_weights,model_weights,model_weights,model_weights,\n",
        "                              ]\n",
        "\n",
        "        #The size of each matrix= [50,12,12]\n",
        "\n",
        "        Matrix_acc_training=np.zeros([100,12,12])\n",
        "        Matrix_acc_test=np.zeros([100,12,12])\n",
        "        Matrix_acc_balanced=np.zeros([100,12,12])\n",
        "        Matrix_acc=np.zeros([100,12,12])\n",
        "        Matrix_loss_training=np.zeros([100,12,12])\n",
        "        Matrix_1=np.zeros([100,12,12])\n",
        "        Matrix_1_new=np.zeros([100,12,12])\n",
        "        training_epoch=0\n",
        "        Cluster_set_r=[]\n",
        "        for Dataset_ID in range(client_number):\n",
        "\n",
        "            Model_ID= Dataset_ID\n",
        "            local_training=sub_train_dataset[Dataset_ID]\n",
        "            local_test=total_test_dataset[Dataset_ID]\n",
        "            local_test_balanced=balanced_test_dataset[Dataset_ID]\n",
        "\n",
        "            local_model.set_weights(Summary_of_local_weights[Model_ID])\n",
        "\n",
        "            #Test and calculate matrie\n",
        "\n",
        "            Model_loss,Model_acc=local_model.evaluate(local_training[:,:sample_len],local_training[:,sample_len], verbose=0)\n",
        "            Matrix_acc_training[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "            Matrix_loss_training[training_epoch,Dataset_ID,Model_ID]=Model_loss\n",
        "\n",
        "            Model_loss,Model_acc=local_model.evaluate(local_test[:,:sample_len],local_test[:,sample_len], verbose=0)\n",
        "            Matrix_acc_test[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "\n",
        "\n",
        "            Model_loss,Model_acc=local_model.evaluate(local_test_balanced[:,:sample_len],local_test_balanced[:,sample_len], verbose=0)\n",
        "            Matrix_acc_balanced[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "\n",
        "        for training_epoch in range(1,50):\n",
        "\n",
        "            for client in range(client_number):\n",
        "                local_training=sub_train_dataset[client]\n",
        "                local_model.set_weights(Summary_of_local_weights[client])\n",
        "                local_model.fit(local_training[:,:sample_len],local_training[:,sample_len],batch_size=batch_size, epochs=5, verbose=0)\n",
        "                Summary_of_local_weights[client]=local_model.get_weights()\n",
        "                print('Client' + str(Dataset_ID)+ 'done')\n",
        "\n",
        "\n",
        "\n",
        "            for i_th in range (client_number):\n",
        "              for j_th in range(client_number):\n",
        "                Matrix_1_new[training_epoch,i_th,j_th]=Cal_model_dis(Summary_of_local_weights[i_th],Summary_of_local_weights[j_th])\n",
        "            cluster_set=[]\n",
        "            clustering = AffinityPropagation(random_state=5).fit(Matrix_1_new[training_epoch])\n",
        "            Type_of_labels=np.unique(clustering.labels_)\n",
        "            for Type_label in Type_of_labels:\n",
        "                cluster_set.append(list(np.where(clustering.labels_==Type_label)[0]))\n",
        "\n",
        "            Cluster_set_r.append(cluster_set)\n",
        "            # print(Matrix_1_new[training_epoch])\n",
        "            print('Clustering_results:')\n",
        "            print(cluster_set)\n",
        "\n",
        "            cluster_set=[]\n",
        "            clustering = AffinityPropagation(random_state=5).fit(Matrix_1_new[training_epoch])\n",
        "            Type_of_labels=np.unique(clustering.labels_)\n",
        "            for Type_label in Type_of_labels:\n",
        "                cluster_set.append(list(np.where(clustering.labels_==Type_label)[0]))\n",
        "            Cluster_set_r.append(cluster_set)\n",
        "            print('Clustering_results:')\n",
        "            print(cluster_set)\n",
        "\n",
        "\n",
        "            #Model aggregation\n",
        "            for sub_cluster in cluster_set:\n",
        "                if len(sub_cluster) !=1:\n",
        "                    print('Model_agg')\n",
        "                    Total_sample_number=np.shape(sub_train_dataset[sub_cluster[0]])[0]\n",
        "                    Updated_local_weights[sub_cluster[0]]=np.multiply(Summary_of_local_weights[sub_cluster[0]],Total_sample_number)\n",
        "                    for Model_ID in sub_cluster[1::]:\n",
        "                        Sample_number=np.shape(sub_train_dataset[Model_ID])[0]\n",
        "                        Total_sample_number= Total_sample_number+Sample_number\n",
        "                        Updated_local_weights[sub_cluster[0]]=Updated_local_weights[sub_cluster[0]]+np.multiply(Summary_of_local_weights[Model_ID],Sample_number)\n",
        "                    for ith_Model in sub_cluster:\n",
        "                        Summary_of_local_weights[ith_Model]=np.divide(Updated_local_weights[sub_cluster[0]], Total_sample_number)\n",
        "\n",
        "\n",
        "            #Verify model performance\n",
        "            for Dataset_ID in range(client_number):\n",
        "                local_training=sub_train_dataset[Dataset_ID]\n",
        "                local_test=total_test_dataset[Dataset_ID]\n",
        "                local_test_balanced=balanced_test_dataset[Dataset_ID]\n",
        "\n",
        "                for Model_ID in range(client_number):\n",
        "                    local_model.set_weights(Summary_of_local_weights[Model_ID])\n",
        "\n",
        "                    #Test and calculate matrie\n",
        "\n",
        "                    Model_loss,Model_acc=local_model.evaluate(local_training[:,:sample_len],local_training[:,sample_len], verbose=0)\n",
        "                    Matrix_acc_training[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "                    Matrix_loss_training[training_epoch,Dataset_ID,Model_ID]=Model_loss\n",
        "\n",
        "\n",
        "                    Model_loss,Model_acc=local_model.evaluate(local_test[:,:sample_len],local_test[:,sample_len], verbose=0)\n",
        "                    Matrix_acc_test[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "\n",
        "\n",
        "                    Model_loss,Model_acc=local_model.evaluate(local_test_balanced[:,:sample_len],local_test_balanced[:,sample_len], verbose=0)\n",
        "                    Matrix_acc_balanced[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "            print(np.diagonal(Matrix_acc_training[training_epoch]))\n",
        "            print('Test:')\n",
        "            print(np.diagonal(Matrix_acc_test[training_epoch]))\n",
        "            print('Bal:')\n",
        "            print(np.diagonal(Matrix_acc_balanced[training_epoch]))\n",
        "\n",
        "    save_data_dir_root=\"/content/mnt/MyDrive/Code_for_bearing_diagnosis/ISUresults/Run\"+str(runs)+\"/FedCos2_all_samples_\"+str(data_split_ratio)\n",
        "    if not os.path.exists(save_data_dir_root):\n",
        "        os.makedirs(save_data_dir_root)\n",
        "    os.chdir(save_data_dir_root)\n",
        "\n",
        "    with open('Summary_of_all_model','wb') as fp:\n",
        "        pickle.dump(Summary_of_local_weights,fp)\n",
        "    savemat('Model_training.mat', {'training_loss':Matrix_loss_training,'training_acc':Matrix_acc_training})\n",
        "    savemat('Model_test_all.mat', {'Test_IID':Matrix_acc_test,'Test_NonIID':Matrix_acc_balanced})\n",
        "    savemat('Model_matrix.mat', {'Matrix1':Matrix_1_new})\n",
        "\n",
        "    with open('Cluster_set.pkl', 'wb') as f:\n",
        "        pickle.dump(Cluster_set_r, f)\n",
        "    finalc = np.empty(len(Cluster_set_r),dtype=object)\n",
        "    finalc[:]=Cluster_set_r\n",
        "    np.save('FedCos2Cluster_set_r.npy', Cluster_set_r, allow_pickle=True)\n",
        "    return print('FedCos2_done')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "etPZz3rHBfCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "def Cal_model_dis(A,B):\n",
        "    Par1=[]\n",
        "    Par2=[]\n",
        "    for i in range(len(A)):\n",
        "        Par1.append(A[i].reshape(-1,1))\n",
        "        Par2.append(B[i].reshape(-1,1))\n",
        "    Par1=np.concatenate(Par1).reshape(-1)\n",
        "    Par2=np.concatenate(Par2).reshape(-1)\n",
        "\n",
        "    return distance.cosine(Par1,Par2)\n"
      ],
      "metadata": {
        "id": "8CjPZkbfO-p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEzgPntdL4Wu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def Local_train(runs):\n",
        "  for data_split_ratio in [1]:#[0.2,0.4,0.6,0.8,1]:\n",
        "      sub_train_dataset=[]\n",
        "      for i in total_train_dataset:\n",
        "          sub_train_dataset.append(subset_dataset(i,data_split_ratio))\n",
        "      #Train model, 20% as validation data to check if model is overfitted.\n",
        "      total_epoch=300\n",
        "      seed(30+runs)\n",
        "      set_random_seed(30+runs)\n",
        "      tf.random.set_seed(30+runs)\n",
        "      resnet_config = dict(num_classes=num_classes, num_layers=num_layers, num_hidden=num_hidden)\n",
        "      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "      metrics = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "      optimizer = SGD(learning_rate=lr,\n",
        "                  # decay=lr_decay,\n",
        "                  momentum=0.9\n",
        "                  )\n",
        "      # optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "      train_config = dict(loss=loss, metrics=metrics, optimizer=optimizer)\n",
        "      local_model = DeepResNetSNGPWithCovReset(**resnet_config)\n",
        "      local_model.compile(**train_config)\n",
        "      local_model.build((None,sample_len))\n",
        "      model_weights = local_model.get_weights()\n",
        "      Summary_of_local_weights=[model_weights,model_weights,model_weights,model_weights,\n",
        "                                model_weights,model_weights,model_weights,model_weights,\n",
        "                                model_weights,model_weights,model_weights,model_weights,\n",
        "                                ]\n",
        "      Updated_local_weights=[model_weights,model_weights,model_weights,model_weights,\n",
        "                            model_weights,model_weights,model_weights,model_weights,\n",
        "                            model_weights,model_weights,model_weights,model_weights,\n",
        "                            ]\n",
        "      #Revise the following part: training epoch for each model = 50. After each epoch, each client load model and calculate and generate matrix.\n",
        "\n",
        "      #The size of each matrix= [50,12,12]\n",
        "      Matrix_acc_training=np.zeros([100,12,12])\n",
        "      Matrix_acc_test=np.zeros([100,12,12])\n",
        "      Matrix_acc_balanced=np.zeros([100,12,12])\n",
        "      Matrix_acc=np.zeros([100,12,12])\n",
        "      Matrix_loss_training=np.zeros([100,12,12])\n",
        "\n",
        "      training_epoch=0\n",
        "      Cluster_set_r=[]\n",
        "      for Dataset_ID in range(client_number):\n",
        "          Model_ID= Dataset_ID\n",
        "\n",
        "          local_training=sub_train_dataset[Dataset_ID]\n",
        "          local_test=total_test_dataset[Dataset_ID]\n",
        "          local_test_balanced=balanced_test_dataset[Dataset_ID]\n",
        "\n",
        "          local_model.set_weights(Summary_of_local_weights[Model_ID])\n",
        "\n",
        "          #Test and calculate matrie\n",
        "          Model_loss,Model_acc=local_model.evaluate(local_training[:,:sample_len],local_training[:,sample_len], verbose=0)\n",
        "          Matrix_acc_training[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "          Matrix_loss_training[training_epoch,Dataset_ID,Model_ID]=Model_loss\n",
        "\n",
        "          Model_loss,Model_acc=local_model.evaluate(local_test[:,:sample_len],local_test[:,sample_len], verbose=0)\n",
        "          Matrix_acc_test[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "\n",
        "\n",
        "          Model_loss,Model_acc=local_model.evaluate(local_test_balanced[:,:sample_len],local_test_balanced[:,sample_len], verbose=0)\n",
        "          Matrix_acc_balanced[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "\n",
        "      for training_epoch in range(1,50):\n",
        "          for client in range(client_number):\n",
        "              local_training=sub_train_dataset[client]\n",
        "              local_model.set_weights(Summary_of_local_weights[client])\n",
        "              local_model.fit(local_training[:,:sample_len],local_training[:,sample_len],batch_size=batch_size, epochs=5, verbose=0)\n",
        "              Summary_of_local_weights[client]=local_model.get_weights()\n",
        "\n",
        "          print(str(training_epoch)+ 'done')\n",
        "\n",
        "          #Verify model performance\n",
        "\n",
        "          for Dataset_ID in range(client_number):\n",
        "              local_training=sub_train_dataset[Dataset_ID]\n",
        "              local_test=total_test_dataset[Dataset_ID]\n",
        "              local_test_balanced=balanced_test_dataset[Dataset_ID]\n",
        "\n",
        "              for Model_ID in range(client_number):\n",
        "                  local_model.set_weights(Summary_of_local_weights[Model_ID])\n",
        "\n",
        "                  #Test and calculate matrie\n",
        "\n",
        "                  Model_loss,Model_acc=local_model.evaluate(local_training[:,:sample_len],local_training[:,sample_len], verbose=0)\n",
        "                  Matrix_acc_training[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "                  Matrix_loss_training[training_epoch,Dataset_ID,Model_ID]=Model_loss\n",
        "\n",
        "\n",
        "                  Model_loss,Model_acc=local_model.evaluate(local_test[:,:sample_len],local_test[:,sample_len], verbose=0)\n",
        "                  Matrix_acc_test[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "\n",
        "\n",
        "                  Model_loss,Model_acc=local_model.evaluate(local_test_balanced[:,:sample_len],local_test_balanced[:,sample_len], verbose=0)\n",
        "                  Matrix_acc_balanced[training_epoch,Dataset_ID,Model_ID]=Model_acc\n",
        "          print(np.diagonal(Matrix_acc_training[training_epoch]))\n",
        "          print('Test:')\n",
        "          print(np.diagonal(Matrix_acc_test[training_epoch]))\n",
        "          print('Bal:')\n",
        "          print(np.diagonal(Matrix_acc_balanced[training_epoch]))\n",
        "\n",
        "\n",
        "  save_data_dir_root=\"/content/mnt/MyDrive/Code_for_bearing_diagnosis/ISUresults_SC3/Run\"+str(runs)+\"/LC_all_samples_\"+str(data_split_ratio)\n",
        "  if not os.path.exists(save_data_dir_root):\n",
        "      os.makedirs(save_data_dir_root)\n",
        "  os.chdir(save_data_dir_root)\n",
        "\n",
        "  with open('Summary_of_all_model','wb') as fp:\n",
        "      pickle.dump(Summary_of_local_weights,fp)\n",
        "  savemat('Model_training.mat', {'training_loss':Matrix_loss_training,'training_acc':Matrix_acc_training})\n",
        "  savemat('Model_test_all.mat', {'Test_IID':Matrix_acc_test,'Test_NonIID':Matrix_acc_balanced})\n",
        "\n",
        "  return print('LC_done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bDcgHQIyyH4k"
      },
      "outputs": [],
      "source": [
        "Fed_Avg(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbROrHzROxkR"
      },
      "outputs": [],
      "source": [
        "Fed_SNGP(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Fed_Cos_2(1)"
      ],
      "metadata": {
        "id": "sGSehageBvdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyPFDGiUJ5vv"
      },
      "outputs": [],
      "source": [
        "Local_train(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OawIGdCLwFD"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBL39w1pL10I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}